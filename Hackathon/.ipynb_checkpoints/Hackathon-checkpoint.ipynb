{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88904c20",
   "metadata": {},
   "source": [
    "# Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dca94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d2d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('./Datasets/training_set_features.csv')\n",
    "train_output_df=pd.read_csv('./Datasets/training_set_labels.csv')\n",
    "test_df=pd.read_csv('./Datasets/test_set_features.csv')\n",
    "submission_format=pd.read_csv('./Datasets/submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b5ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, test_df, train_output_df, test_output_df = train_test_split(training_df, training_output_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a018259",
   "metadata": {},
   "source": [
    "# Understanding Data and Cleaning Data, Standardization Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4347bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Print Numbers of Rows and Columns in the Train Dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows,Columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.head()\n",
    "#Print Numbers of Rows and Columns in the Train Dataset\n",
    "print(\"Rows,Columns:\",train_df.shape)\n",
    "\n",
    "#Finding the NUll Columns for Data Cleaning\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints Datatype of each column\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e504679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Data and Null Values We decided to remove employment_industry and employment_education\n",
    "columns_to_drop = ['employment_industry', 'employment_occupation']\n",
    "train_df.drop(columns=columns_to_drop, inplace=True)\n",
    "test_df.drop(columns=columns_to_drop,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns of Dataframe\n",
    "remaining_col=train_df.columns\n",
    "for i in remaining_col:\n",
    "    print(i,\":\",train_df[i].unique())\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "columns = train_df.columns\n",
    "print(columns)\n",
    "\n",
    "# Fill NaN values in categorical columns with a placeholder i.e. Mode Value\n",
    "for col in columns:\n",
    "    mode_value = train_df[col].mode()[0]\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value= mode_value)\n",
    "    train_df[[col]] = imputer.fit_transform(train_df[[col]])\n",
    "\n",
    "for col in columns:\n",
    "    mode_value = test_df[col].mode()[0]\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value= mode_value)\n",
    "    test_df[[col]] = imputer.fit_transform(test_df[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = train_df.select_dtypes(include=['object']).columns\n",
    "print(object_columns)\n",
    "\n",
    "#Using One Hot Encoder to Encode the Object Data Columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder on the categorical columns\n",
    "onehot_encoded = onehot_encoder.fit_transform(train_df[object_columns])\n",
    "onehot_encoded_test = onehot_encoder.fit_transform(test_df[object_columns])\n",
    "\n",
    "# Create a DataFrame to view the encoded data\n",
    "encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(object_columns))\n",
    "encoded_test_df = pd.DataFrame(onehot_encoded_test, columns=onehot_encoder.get_feature_names_out(object_columns))\n",
    "\n",
    "# Concatenate with the original DataFrame\n",
    "train_df = pd.concat([train_df.drop(columns=object_columns), encoded_df], axis=1)\n",
    "test_df = pd.concat([test_df.drop(columns=object_columns), encoded_test_df], axis=1)\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(train_df.shape)\n",
    "train_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac8ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nnull_cols=train_df.isnull().sum()\n",
    "for col, count in nnull_cols.items():\n",
    "    print(f\"{col}: {count}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnull_cols_op=train_output_df.isnull().sum()\n",
    "for col, count in nnull_cols_op.items():\n",
    "    print(f\"{col}: {count}\")\n",
    "train_df.head()\n",
    "#Train Data Output have all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70227dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnull_test_cols=test_df.isnull().sum()\n",
    "for col, count in nnull_test_cols.items():\n",
    "    print(f\"{col}: {count}\")\n",
    "test_df.head()\n",
    "\n",
    "print(test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f320124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_output_df.columns:\n",
    "    print(train_output_df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Highly Corelated Columns for Less Impact on Results\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = train_df.corr()\n",
    "\n",
    "# We are interested in absolute correlation values above a certain threshold\n",
    "threshold = 0.95\n",
    "high_corr_pairs = np.where(np.abs(corr_matrix) > threshold)\n",
    "high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) for x, y in zip(*high_corr_pairs) if x != y and x < y]\n",
    "\n",
    "# Remove one column from each pair\n",
    "columns_to_remove = set()\n",
    "for col1, col2 in high_corr_pairs:\n",
    "    columns_to_remove.add(col2)\n",
    "\n",
    "# Remove the identified columns\n",
    "train_df = train_df.drop(columns=list(columns_to_remove))\n",
    "test_df = test_df.drop(columns=list(columns_to_remove))\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(\"Columns  related:\",high_corr_pairs)\n",
    "print(\"Remaining columns:\", train_df.columns)\n",
    "print(train_df.head())\n",
    "\n",
    "#But Columns were just complement of each other so removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training data\n",
    "train_df = scaler.fit_transform(train_df)\n",
    "\n",
    "# Transform the testing data using the same scaler\n",
    "test_df = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Naive Bayes For Probabilities\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "#xyz vaccine\n",
    "model.fit(train_df, train_output_df['xyz_vaccine'])\n",
    "\n",
    "probability_xyz = model.predict_proba(test_df)[:, 1]\n",
    "submission_format['h1n1_vaccine']=probability_xyz.round(4)\n",
    "\n",
    "\n",
    "#seasonal_vaccine\n",
    "\n",
    "model_sv = GaussianNB()\n",
    "model_sv.fit(train_df,train_output_df['seasonal_vaccine'])\n",
    "\n",
    "probability_sv = model_sv.predict_proba(test_df)[:, 1]\n",
    "submission_format['seasonal_vaccine']=probability_sv.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad8945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474b918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd13a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_log = LogisticRegression()\n",
    "\n",
    "#xyz vaccine\n",
    "model_log.fit(train_df, train_output_df['xyz_vaccine'])\n",
    "\n",
    "probability_xyz_log = model_log.predict_proba(test_df)[:, 1]\n",
    "submission_format['log_h1n1_vaccine']=probability_xyz_log.round(4)\n",
    "\n",
    "\n",
    "#seasonal_vaccine\n",
    "\n",
    "model_sv_log = GaussianNB()\n",
    "model_sv_log.fit(train_df,train_output_df['seasonal_vaccine'])\n",
    "\n",
    "probability_sv_log = model_sv.predict_proba(test_df)[:, 1]\n",
    "submission_format['log_seasonal_vaccine']=probability_sv_log.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_format.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc1973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
